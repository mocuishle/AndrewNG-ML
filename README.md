

## About the course & author

Please see the [coursera](https://www.coursera.org/learn/machine-learning/home/info/README.md)
Taught by [Andrew Ng](https://www.coursera.org/instructor/andrewng/README.md)

## EX1
单变量、多变量的线性回归（有监督）：梯度下降  常规线性变换（Normal Equation）


### 梯度下降

1.选择预测方程类型（目前是线性），设置初始参数，并根据观测数据对变量进行归一化

2.根据MSE，计算出代价方程，并设置学习速率参数和迭代次数

3.根据预测方程、初始参数、观测数据、代价方程，对参数进行更新（参数更新方程）

4.根据代价方程降低速度，调整学习速率参数，直至迭代出代价方程最低值，并选择对应参数作为最终结果

### Normal Equation

原理 ： 将预测方程两端与变量的转置矩阵相乘，求解可得残差最小时的参数值。

需要注意：
    
        变量矩阵的內吉不可逆有两种情况 ：重复的特征（例如面积用英尺表达和用米表达作为两个特征）或特征过多
        
        解决：去掉部分特征，或者对变量进行正则化


## EX2
分类算法、Logistics回归分析、正则化应用

### 分类算法

将目标输出值离散化，并与对应的结果相映射。需要引用sigmoid函数来让预测的函数能够连续（方便后续观察、计算），具体形式 y=1/（1+e^(theta'*X)

预测函数依旧为连续函数，输出值可以看作样本为正例的概率大小

### Logistics回归

预测函数涉及到对数概率的操作，因此需要用Logistics回归来进行分析

原理：最大似然估计（MLE），略（后续会涉及到。。。）
回归方法：依旧是 梯度下降 和 Normal Equation（与线性回归最大的区别就是预测函数的形式有所变化，更新函数和代价函数也发生相应的变化）

### 正则化

如之前所说，引入过多的特征变量，会导致过耦合，正则化则是要减弱耦合的情况

正则化代价函数 = 原代价函数 + 正则参数*(theta.*theta)/（2*m）；

正则化代价函数最小化时兼顾了预测准确性和参数值的大小，后续参数值越小耦合程度会越低

## EX3

### 多个分类任务下的logistics回归

方法： one vs all，一次迭代选择一个分类作为正例

回归过程：与单一分类任务回归过程基本一致。

（与单一回归）差别 ：向量除了原来的维度之外还要考虑预测值向量的维度，在计算过程中要重视

### 利用神经网络分析手写数字

原理：模拟人体神经系统，根据阈值和非线性分类器来处理复杂的问题

优点：可以解决很多线性回归不能处理的非线性情况（例如异或），特征数量过多时

相关概念 ：输入层、隐藏层、输出层，权值网络，偏置神经元

建立过程 ：前向传播（暂时接触到这种），向量化运算
